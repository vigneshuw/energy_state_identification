{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b041700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:13.777557Z",
     "start_time": "2021-07-28T05:07:13.760795Z"
    }
   },
   "outputs": [],
   "source": [
    "# To enable faster auto-complete\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808a12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:15.538962Z",
     "start_time": "2021-07-28T05:07:14.263722Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "from utils import data_augmentation\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79885e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:17.973342Z",
     "start_time": "2021-07-28T05:07:15.995197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting the GPU to be used \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    # Restrict tensor flow to use GPU-1\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0:4], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Set GPUs before initializing\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249e1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:17.997150Z",
     "start_time": "2021-07-28T05:07:17.993153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting of the confusion matrix\n",
    "def confusion_matrix(model, X_test, y_test, class_names):\n",
    "    # Prediction on test data set -> predicting classes\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n",
    "    # Normalizing the confusion matrix\n",
    "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1) [:, np.newaxis], decimals=2)\n",
    "    con_mat_df = pd.DataFrame(con_mat_norm, index=class_names, columns=class_names)\n",
    "    # Plotting using an heat map\n",
    "    figure = plt.figure(figsize=(8,8))\n",
    "    sns.set(font_scale = 2)\n",
    "    sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef3804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:18.251451Z",
     "start_time": "2021-07-28T05:07:18.246155Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_create_dir(*args):\n",
    "    # Get the directory name\n",
    "    temp = \"\"\n",
    "    for i in args:\n",
    "        temp = os.path.join(temp, i)\n",
    "    \n",
    "    # Check if exists else create them all\n",
    "    if os.path.isdir(temp):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de1e3c",
   "metadata": {},
   "source": [
    "# Multi-output prediction model\n",
    "\n",
    "- A single model identifies the axis (classification) and predicts the feed rate (regression)\n",
    "\n",
    "- The axis that are detected are given below\n",
    "\n",
    "    - X-axis\n",
    "    - Y-axis \n",
    "    - Z-axis\n",
    "    - B-axis\n",
    "    - C-axis\n",
    "\n",
    "- The feed rates predicted involves the whole range for that particular axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447ea15",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fead5b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:20.853300Z",
     "start_time": "2021-07-28T05:07:20.825518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple model architecture\n",
    "def build_multipred_model(input_shape, act_fn, compile_model=True):\n",
    "    \n",
    "    # Build the model architecture with multiple outputs\n",
    "    input_layer = keras.Input(shape=input_shape, name=\"input\")\n",
    "    x = keras.layers.Conv1D(1024, 3, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(input_layer)\n",
    "    x = keras.layers.Conv1D(512, 3, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPool1D(2)(x)\n",
    "    x = keras.layers.Conv1D(512, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    x = keras.layers.Conv1D(256, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    # Where the split between two output happens\n",
    "    split_layer = keras.layers.MaxPool1D(2)(x)\n",
    "    x = keras.layers.Flatten()(split_layer)\n",
    "    x = keras.layers.Dense(512, activation=act_fn[0], kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    x = keras.layers.Dropout(0.7)(x)\n",
    "    x = keras.layers.Dense(256, activation=act_fn[0], kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    axis_detection = keras.layers.Dense(5, activation=act_fn[1], name=\"axis_detection\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(x)\n",
    "    y = keras.layers.Conv1D(256, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(split_layer)\n",
    "    y = keras.layers.Conv1D(128, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.MaxPool1D(2)(y)\n",
    "    y = keras.layers.Conv1D(128, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.Conv1D(64, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.MaxPool1D(2)(y)\n",
    "    y = keras.layers.Conv1D(64, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.Conv1D(32, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.MaxPool1D(2)(y)\n",
    "    y = keras.layers.Conv1D(32, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.Conv1D(16, 2, activation=act_fn[0], padding=\"same\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.BatchNormalization()(y)\n",
    "    y = keras.layers.MaxPool1D(2)(y)\n",
    "    y = keras.layers.Flatten()(y)\n",
    "    y = keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    y = keras.layers.Dropout(0.5)(y)\n",
    "    y = keras.layers.Dense(256, kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001))(y)\n",
    "    feed_rate_pred = keras.layers.Dense(1, activation=None, name=\"feed_rate_prediction\")(y)\n",
    "    \n",
    "    # Get the model out\n",
    "    model = keras.Model(input_layer, [axis_detection, feed_rate_pred], name=\"multi_output_model\")\n",
    "    \n",
    "    if compile_model:\n",
    "        # Optimizer\n",
    "        adam = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile(optimizer=adam, loss={\"axis_detection\": \"sparse_categorical_crossentropy\", \"feed_rate_prediction\": \"mse\"}, \n",
    "                      metrics={\"axis_detection\": \"accuracy\"}, loss_weights=[1, 20])\n",
    "        \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec765cae",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4309510",
   "metadata": {},
   "source": [
    "### Load without oversampling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0546104f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T17:08:06.840153Z",
     "start_time": "2021-07-21T17:08:06.123404Z"
    }
   },
   "source": [
    "# Directory of the dataset\n",
    "data_dir = os.path.join(os.getcwd(), \"model_data\", \"multi_output_axisfr\")\n",
    "\n",
    "for index1, file in enumerate(os.listdir(data_dir)):\n",
    "    data = np.load(os.path.join(data_dir, file), allow_pickle=True)[()]\n",
    "    \n",
    "    for index2, ((axis, feed_rate), segmented_points) in enumerate(data.items()):\n",
    "        \n",
    "        temp_axis = np.repeat(axis, segmented_points.shape[0])[:, np.newaxis]\n",
    "        temp_fr = np.repeat(feed_rate, segmented_points.shape[0])[:, np.newaxis]\n",
    "    \n",
    "        # part of X and y\n",
    "        if index2 == 0:\n",
    "            # part of y for an axis and all feed rate\n",
    "            part_y_axis = temp_axis\n",
    "            part_y_fr = temp_fr\n",
    "            # part of X\n",
    "            part_X = segmented_points\n",
    "        else:\n",
    "            part_y_axis = np.append(part_y_axis, temp_axis, axis=0)\n",
    "            part_y_fr = np.append(part_y_fr, temp_fr, axis=0)\n",
    "            part_X = np.append(part_X, segmented_points, axis=0)\n",
    "            \n",
    "    if index1 == 0:\n",
    "        # y\n",
    "        y_axis = part_y_axis\n",
    "        y_fr = part_y_fr\n",
    "        # X\n",
    "        X = part_X\n",
    "    else:\n",
    "        y_axis = np.append(y_axis, part_y_axis, axis=0)\n",
    "        y_fr = np.append(y_fr, part_y_fr, axis=0)\n",
    "        X = np.append(X, part_X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db5269",
   "metadata": {},
   "source": [
    "### Load with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a705a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:36.682669Z",
     "start_time": "2021-07-28T05:07:23.868394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory of the dataset\n",
    "data_dir = os.path.join(os.getcwd(), \"model_data\", \"multi_output_axisfr\")\n",
    "\n",
    "for index1, file in enumerate(os.listdir(data_dir)):\n",
    "    data = np.load(os.path.join(data_dir, file), allow_pickle=True)[()]\n",
    "    \n",
    "    # Random oversample the data\n",
    "    data_ov = data_augmentation.naive_resampler(data, 1)\n",
    "    \n",
    "    # Ensure the count for all classes\n",
    "    sys.stdout.write(20 * \"=\" + \"\\n\")\n",
    "    sys.stdout.write(f\"The name of the file - {file} \\n\")\n",
    "    for key, value in data_ov.items():\n",
    "        sys.stdout.write(f\"The class {key} has a shape of {value.shape[0]}\\n\")\n",
    "    \n",
    "    for index2, ((axis, feed_rate), segmented_points) in enumerate(data_ov.items()):\n",
    "        \n",
    "        temp_axis = np.repeat(axis, segmented_points.shape[0])[:, np.newaxis]\n",
    "        temp_fr = np.repeat(feed_rate, segmented_points.shape[0])[:, np.newaxis]\n",
    "    \n",
    "        # part of X and y\n",
    "        if index2 == 0:\n",
    "            # part of y for an axis and all feed rate\n",
    "            part_y_axis = temp_axis\n",
    "            part_y_fr = temp_fr\n",
    "            # part of X\n",
    "            part_X = segmented_points\n",
    "        else:\n",
    "            part_y_axis = np.append(part_y_axis, temp_axis, axis=0)\n",
    "            part_y_fr = np.append(part_y_fr, temp_fr, axis=0)\n",
    "            part_X = np.append(part_X, segmented_points, axis=0)\n",
    "            \n",
    "    if index1 == 0:\n",
    "        # y\n",
    "        y_axis = part_y_axis\n",
    "        y_fr = part_y_fr\n",
    "        # X\n",
    "        X = part_X\n",
    "    else:\n",
    "        y_axis = np.append(y_axis, part_y_axis, axis=0)\n",
    "        y_fr = np.append(y_fr, part_y_fr, axis=0)\n",
    "        X = np.append(X, part_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa4a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:36.927324Z",
     "start_time": "2021-07-28T05:07:36.924425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make the generator for the data \n",
    "def make_generator(X, y_axis, y_fr, n_splits):\n",
    "    \n",
    "    def gen():\n",
    "        for train_index, test_index in KFold(n_splits).split(X):\n",
    "            X_train, y_axis_train, y_fr_train = X[train_index], y_axis[train_index], y_fr[train_index]\n",
    "            X_test, y_axis_test, y_fr_test = X[test_index], y_axis[test_index], y_fr[test_index]\n",
    "            \n",
    "            # Yield the data every iteration\n",
    "            yield X_train, X_test, y_axis_train, y_axis_test, y_fr_train, y_fr_test\n",
    "    \n",
    "    # Return as a tf dataset generator API\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64, tf.float64, tf.int8, tf.int8, tf.float16, tf.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba24716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:37.467089Z",
     "start_time": "2021-07-28T05:07:37.112633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffling the data. This is pretty important, otherwise the model will fail.\n",
    "X, y_axis, y_fr = shuffle(X, y_axis, y_fr, random_state=42)\n",
    "\n",
    "# Create the Cross-Validation dataset\n",
    "cv_dataset = make_generator(X, y_axis, y_fr, n_splits=10)\n",
    "\n",
    "# Split into testing data\n",
    "# X, X_test, y_axis, y_axis_test, y_fr, y_fr_test = train_test_split(X, y_axis, y_fr, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92af6a",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc353338",
   "metadata": {},
   "source": [
    "### Using multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb74a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T05:07:39.015717Z",
     "start_time": "2021-07-28T05:07:37.652013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using multiple GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    # Build the model with appropriate parameters\n",
    "    model = build_multipred_model((85, 3), (\"relu\", \"softmax\"), compile_model=False)\n",
    "\n",
    "# Optimizer\n",
    "adam = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# Compilation\n",
    "model.compile(optimizer=adam, loss={\"axis_detection\": \"sparse_categorical_crossentropy\", \"feed_rate_prediction\": \"mse\"}, \n",
    "              metrics={\"axis_detection\": \"accuracy\"}, loss_weights=[1, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ffde9b",
   "metadata": {},
   "source": [
    "### Using single GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a22a49c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T16:55:32.699680Z",
     "start_time": "2021-07-21T16:55:32.232729Z"
    }
   },
   "source": [
    "# Using a single GPU\n",
    "\n",
    "# Build the model with appropriate parameters\n",
    "model = build_multipred_model((85, 3), (\"relu\", \"softmax\"))\n",
    "# Print the model summary for verification\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1d8b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-28T05:07:46.669Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=100)\n",
    "\n",
    "# save dir name\n",
    "dir_name = \"multi_output_ax-fr\"\n",
    "save_path = os.path.join(os.getcwd(), \"model_weights\", dir_name)\n",
    "folder_time = datetime.datetime.now().isoformat()\n",
    "\n",
    "# Create all required directories\n",
    "check_create_dir(save_path, folder_time)\n",
    "\n",
    "history = {}\n",
    "score_eval = {}\n",
    "for fold, (X_train, X_test, y_axis_train, y_axis_test, y_fr_train, y_fr_test) in enumerate(cv_dataset):\n",
    "    \n",
    "    if fold == 0:\n",
    "        # Resetting model weights every iteration\n",
    "        initial_weights = model.get_weights()\n",
    "        # Divert the output to console out\n",
    "        nb_stdout = sys.stdout\n",
    "\n",
    "    sys.stdout = open(\"/dev/stdout\", \"w\")\n",
    "\n",
    "    # Fit the model and save the training progress\n",
    "    history[fold] = model.fit(X_train, {\"axis_detection\": y_axis_train, \"feed_rate_prediction\": y_fr_train}, batch_size=1024, epochs=5000, \n",
    "                        shuffle=True, validation_split=0.30, callbacks=[early_stopping_cb])\n",
    "    # Bring the output back to the notebook\n",
    "    sys.stdout = nb_stdout\n",
    "    \n",
    "    # Print the test results\n",
    "    score_eval[fold] = model.evaluate(X_test, {\"axis_detection\": y_axis_test, \"feed_rate_prediction\": y_fr_test})\n",
    "    print(30*\"=\")\n",
    "    print(f\"The results for fold-{fold} is {score_eval[fold]}\")\n",
    "    \n",
    "    # Save the model weights \n",
    "    model.save(os.path.join(save_path, folder_time,  f\"multi-output_KFold-{fold}_model.h5\"))\n",
    "    # Rest the model weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f5678",
   "metadata": {},
   "source": [
    "## Saving the model training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12497bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:38:14.509963Z",
     "start_time": "2021-07-30T06:38:14.495440Z"
    }
   },
   "outputs": [],
   "source": [
    "# The the training history of the model is saved for future reference\n",
    "save_location = os.path.join(os.getcwd(), \"model_weights\", \"multi_output_ax-fr\", folder_time, \"training_history\")\n",
    "\n",
    "# history - dict\n",
    "new_history = {}\n",
    "for key, value in history.items():\n",
    "    new_history[key] = value.history\n",
    "    \n",
    "with open(os.path.join(save_location, \"history.pickle\"), \"wb\") as fhandle:\n",
    "    pickle.dump(new_history, fhandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# score - dict\n",
    "with open(os.path.join(save_location, \"score.pickle\"), \"wb\") as fhandle:\n",
    "    pickle.dump(score_eval, fhandle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902f114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T19:38:02.573232Z",
     "start_time": "2021-07-12T19:14:17.750Z"
    }
   },
   "source": [
    "## Plotting the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d014b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:38:54.481029Z",
     "start_time": "2021-07-30T06:38:54.469765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the saved training history if required\n",
    "load_location = os.path.join(os.getcwd(), \"model_weights\", \"multi_output_ax-fr\", folder_time, \"training_history\", \"history.pickle\")\n",
    "with open(load_location, \"rb\") as fhandle:\n",
    "    loaded_history = pickle.load(fhandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afac35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:38:55.955735Z",
     "start_time": "2021-07-30T06:38:55.778957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the axis detection loss\n",
    "plt.plot(loaded_history[0][\"axis_detection_loss\"])\n",
    "plt.plot(loaded_history[0][\"val_axis_detection_loss\"])\n",
    "plt.title('Model - Axis detection loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cd030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:38:59.286597Z",
     "start_time": "2021-07-30T06:38:59.138232Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "# Plotting the axis detection accuracy\n",
    "plt.plot(loaded_history[0][\"axis_detection_accuracy\"])\n",
    "plt.plot(loaded_history[0][\"val_axis_detection_accuracy\"])\n",
    "plt.title('Model - Axis detection accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e748e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:39:01.348069Z",
     "start_time": "2021-07-30T06:39:01.211350Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "# Plotting the feed rate prediction loss\n",
    "plt.plot(loaded_history[0][\"feed_rate_prediction_loss\"])\n",
    "plt.plot(loaded_history[0][\"val_feed_rate_prediction_loss\"])\n",
    "plt.title('Model - Feed rate prediction loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f584273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T06:39:03.701972Z",
     "start_time": "2021-07-30T06:39:03.547256Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "# Plotting the cumulative loss\n",
    "plt.plot(loaded_history[0][\"loss\"])\n",
    "plt.plot(loaded_history[0][\"val_loss\"])\n",
    "plt.title('Model - The cummulative loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15cf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
